{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6irZIe4L5mna"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Reshape, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# Load MNIST data\n",
        "(X_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values to be between -1 and 1\n",
        "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(1000)\n",
        "\n",
        "# Define the generator model\n",
        "def build_generator(latent_dim):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(128, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(784, activation='tanh'))\n",
        "    model.add(Reshape((28, 28, 1)))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the discriminator model\n",
        "def build_discriminator(img_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Flatten(input_shape=img_shape))\n",
        "    model.add(Dense(128))\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def save_generated_images(epoch, examples=10, dim=(1, 10), figsize=(10, 1)):\n",
        "    noise = np.random.normal(0, 1, (examples, latent_dim))\n",
        "    generated_imgs = generator.predict(noise)\n",
        "\n",
        "    generated_imgs = 0.5 * generated_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(dim[0], dim[1], figsize=figsize)\n",
        "    for i in range(dim[0] * dim[1]):\n",
        "        axs[i].imshow(generated_imgs[i, :, :, 0], cmap='gray')\n",
        "        axs[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"gan_generated_image_epoch_{epoch}.png\")\n",
        "    plt.show()\n",
        "# Build and compile the discriminator\n",
        "img_shape = (28, 28, 1)\n",
        "discriminator = build_discriminator(img_shape)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "# Build the generator\n",
        "latent_dim = 100\n",
        "generator = build_generator(latent_dim)\n",
        "\n",
        "# Build the GAN model\n",
        "discriminator.trainable = False\n",
        "gan_input = layers.Input(shape=(latent_dim,))\n",
        "x = generator(gan_input)\n",
        "gan_output = discriminator(x)\n",
        "\n",
        "gan = Model(gan_input, gan_output)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64\n",
        "epochs = 15000\n",
        "sample_interval = 1000\n",
        "\n",
        "# Training the GAN\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # Train discriminator\n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    real_imgs = X_train[idx]\n",
        "\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    fake_imgs = generator.predict(noise)\n",
        "\n",
        "    d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((batch_size, 1)))\n",
        "    d_loss_fake = discriminator.train_on_batch(fake_imgs, np.zeros((batch_size, 1)))\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    valid_labels = np.ones((batch_size, 1))\n",
        "\n",
        "    g_loss = gan.train_on_batch(noise, valid_labels)\n",
        "\n",
        "    # Print progress\n",
        "    if epoch % sample_interval == 0:\n",
        "        print(f\"Epoch {epoch}, D Loss: {d_loss[0]}, G Loss: {g_loss}\")\n",
        "\n",
        "        # Save generated images\n",
        "        save_generated_images(epoch)"
      ],
      "metadata": {
        "id": "VXa_DL575xH6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}